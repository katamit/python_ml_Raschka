{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelizing Neural Network Training with TensorFlow\n",
    "\n",
    "- Since its release in __ November 2015, __ TensorFlow has gained a lot of popluarity among machine learning researcher.\n",
    "- While TensorFlow can be considered a low-level deep learning library, simplifying API such as Keras have been developed that make the construction of common deep learning models even  convenient.\n",
    "\n",
    "- TensorFlow can speed up our machine learning tasks significantly. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow and training peformance.\n",
    "\n",
    "![Price and no. of cores comparison of cpu and gpu](cpv_vs_gpu.png)\n",
    "\n",
    "(Date august 2017)\n",
    "\n",
    "At 70 perent of the price of a modern CPU, we got a GPU that has __ 450 times more cores__ and is capable of around __15 times__ more floating-point calcualtiongs per second. So what is holding us back from utilizing GPUs for our machine learning tasks?\n",
    "\n",
    "The challenge is that writing code to target GPUs is not as simple as executing Python code in our interpreter. There are special packages, such as __ CUDA and openCL, that allow us to target the GPU.__ However , writing code in CUDA or OpenCL  is probably not the most convenient environment for implemeting and running machine leanring algorithms. The good news is that this is what TensorFlow was developed for!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is TensorFlow?\n",
    "\n",
    "<font style=\"color:#aa2eb3\">TensorFlow is a scalable and multiplatform programming interface for implementing and running machine learning algorithms, including convenience wrapper for deep learning </font>\n",
    "\n",
    "To improve the performance of training machine learning models, TensorFlow allows execution on both CPUs and GPUs. However its greatest perforamcne capabilities can be discovered using GPUs.\n",
    "\n",
    "<font style=\"color:#aa2eb3\">TensorFlow computations rely on constructiong a directed graph for representing the data flow. Even though building the graph may sound complicated, TensorFlow comes with high-level APIs that has made it very easy.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with low-level TensoFlow API. while implemeting models at this level can be alittle bit cumberosme at first, lthe advantage of the low-level API  si taht it gives us mor frelixbility as progammers to comne the basic oprations and evelp complex macine learning modesl. \n",
    "\n",
    "Starting TensorFlow 1.1.0 high level APIs are added on top of the low-level API(the so-called Layers and Estimators), which allows building and prototyping models much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color:#e6020e\">TensorFlow is built around a computation graph composed of a set of nodes. Each node represents an operation that may have zero or more input or output. The values that flow through the edges of the computation graph are called __tensors__</font>\n",
    "\n",
    "__Tensors__ can be understood as a __generalization of scalars, vectors, matrices and so on.__ More concretely, ascalar can be dfined as a rank-0 tensor, a vector as a rank-1 tensor, a matrix as a rank-2 tensor, and matrices stacked in a third dimension as rank-3 tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> $ z \\space = \\space w \\space X \\space x + b$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x=  1.0 --> z =  3.0\n",
      " x=  0.6 --> z =  2.2\n",
      " x= -1.8 --> z = -2.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a graph\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(dtype = tf.float32, shape =(None), name='input_x')\n",
    "    w = tf.Variable(2.0, name='weight')\n",
    "    b = tf.Variable(1.0, name='bias')\n",
    "    z = tf.add(tf.multiply(w,x) ,b )\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t in [1.0,0.6,-1.8]:\n",
    "        print(' x= %4.1f --> z = %4.1f' %(t, sess.run(z, feed_dict={x:t})))\n",
    "    \n",
    "\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In general, when we develp a model in the TensorFlow low-level API, we need to define placeholders for iinput data(x, y, and sometimes other tunable parameters); then , define the weight matrices and build the model form input to ouptut.\n",
    "- It this is an optimization problem we should degine the loss or csot ffunciton and etermine which optimation algorithm to use.\n",
    "\n",
    "- TensorFlow will create a graph that contains all the sumbols that we have defined as nodes in this graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.          2.20000005 -2.5999999 ]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(z, feed_dict={x: [1.0, 0.6, -1.8]}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Official TensorFlow style gid recommend usign tow-characer spacing for code indents. However here we chose foru character for inetns as it sis more cosnistent with tht officila PYthon style guide and aslo help sin displayig the code syntax higlighting in many text editors correctly as well as the accompanying Jupyter code notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with array structures:\n",
    "Let's dicuss how to use array sturctures in TensorFlow. By executing the following code , we wil crate a simpler rank-3 tensor of size _batchsizex2x3_ , reshape it, and calcuate the column sums using TensorFlow's optmized expressions.  since we don't know the bactch size a priori, we specify _NOne_ for the batch size in the argument for the _shape_ parameter of the placeholder _z_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape :  (3, 2, 3)\n",
      "Reshape \n",
      " [[  0.   1.   2.   3.   4.   5.]\n",
      " [  6.   7.   8.   9.  10.  11.]\n",
      " [ 12.  13.  14.  15.  16.  17.]]\n",
      "Column Sums : \n",
      " [ 18.  21.  24.  27.  30.  33.]\n",
      "Columns Means: \n",
      " [  6.   7.   8.   9.  10.  11.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=(None,2,3), name='input_x')\n",
    "    x2 = tf.reshape(x, shape=(-1,6),name='x2')\n",
    "    \n",
    "    ## Calculate the sum of each columns\n",
    "    xsum = tf.reduce_sum(x2, axis=0, name='col_sum')\n",
    "    \n",
    "    ## Calculate the mean of each columns\n",
    "    xmean = tf.reduce_mean(x2, axis=0, name='col_mean')\n",
    "    \n",
    "with tf.Session(graph=g) as sess:\n",
    "    X_array = np.arange(18).reshape(3,2,3)\n",
    "    \n",
    "    \n",
    "    print('input shape : ', X_array.shape)\n",
    "    print('Reshape \\n', sess.run(x2, feed_dict={x:X_array}))\n",
    "    print('Column Sums : \\n', sess.run(xsum, feed_dict={x:X_array}))\n",
    "    print('Columns Means: \\n', sess.run(xmean, feed_dict={x:X_array}))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we worked with three function  tf.reshape, tf.reduce_sum , tf.reduce_mean . When reshaping a tensor, if you use -1 for a specfic dimension, the size of the that diemsion will be compute daccoridng to the total size ofthe esnor andthe remaining dismension.\n",
    "- Therefore _tf.reshape(tensor, shape=(-1,))_ can be used to flatten a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing a simple model with the low-level TensorFlow API\n",
    "\n",
    "Let's implemen _ordingary Least Square_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train = np.arange(10).reshape((10,1))\n",
    "y_train = np.array([1.0, 1.3, 3.1, 2.0, 5.0, 6.3, 6.6, 7.4, 8.0, 9.0])\n",
    "\n",
    "# Given the dataset, we want to train a linear regression model to predict the output y from the input x.\n",
    "\n",
    "class TfLinreg(object):\n",
    "    def __init__(self, x_dim, learning_rate=0.01, random_seed=None):\n",
    "        self.x_dim = x_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.g = tf.Graph()\n",
    "        \n",
    "        ## Build the model\n",
    "        with self.g.as_default():\n",
    "            ## set graph-level; random -seed\n",
    "            tf.set_random_seed(random_seed)\n",
    "            \n",
    "            self.build()\n",
    "            ## create initializer\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "    def build(self):\n",
    "        ## define placeholders for inputs\n",
    "        self.X = tf.placeholder(dtype=tf.float32,\n",
    "                               shape=(None,self.x_dim),\n",
    "                               name='x_input')\n",
    "        self.y = tf.placeholder(dtype=tf.float32,\n",
    "                               shape=(None),\n",
    "                               name='y_input')\n",
    "        \n",
    "        print(self.X)\n",
    "        print(self.y)\n",
    "        ## define weight matrix and bias vector\n",
    "        w = tf.Variable(tf.zeros(shape=(1)), name='weight')\n",
    "        b = tf.Variable(tf.zeros(shape=(1)), name='bias')\n",
    "        print(w)\n",
    "        print(b)\n",
    "        \n",
    "        self.z_net = tf.squeeze(w*self.X + b, name='z_net')\n",
    "        print(self.z_net)\n",
    "        \n",
    "        sqr_errors = tf.square(self.y - self.z_net, name='sqr_errors')\n",
    "        print(sqr_errors)\n",
    "        \n",
    "        self.mean_cost = tf.reduce_mean(sqr_errors, name='mean_cost')\n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate, name='GradientDescent')\n",
    "        self.optimizer = optimizer.minimize(self.mean_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"x_input:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"y_input:0\", dtype=float32)\n",
      "<tf.Variable 'weight:0' shape=(1,) dtype=float32_ref>\n",
      "<tf.Variable 'bias:0' shape=(1,) dtype=float32_ref>\n",
      "Tensor(\"z_net:0\", dtype=float32)\n",
      "Tensor(\"sqr_errors:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "lrmodel = TfLinreg(X_train.shape[1], learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_linreg(sess, model, X_train, y_train, num_epochs=10):\n",
    "    ## Initialize all vairbles : W and b\n",
    "    sess.run(model.init_op)\n",
    "    \n",
    "    \n",
    "    training_cost = []\n",
    "    for i in range(num_epochs):\n",
    "        _, cost = sess.run([model.optimizer, model.mean_cost], \n",
    "                          feed_dict={model.X:X_train, model.y:y_train})\n",
    "        training_cost.append(cost)\n",
    "    return training_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session(graph=lrmodel.g)\n",
    "\n",
    "training_costs = train_linreg(sess, lrmodel, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbYAAAEmCAYAAAAOb7UzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6aed259eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, len(training_costs) + 1), training_costs )\n",
    "plt.tight_layout()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training cost')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This simple model converges very quickly after a few epochs. Looking a the cost funciton, it seems that we buld a working regression model form this particular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_linreg(sess, model, X_test):\n",
    "    y_pred = sess.run(model.z_net, feed_dict={model.X:X_test})\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSkREIpICSkREItL/D9GBZXUZ61UgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6aecc94f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_train, y_train, marker='s', s=50, label='Training Data')\n",
    "plt.plot(range(X_train.shape[0]), predict_linreg(sess, lrmodel, X_train), color='gray', marker='o',\n",
    "        markersize=6, linewidth=3, label='LinReg model')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Training Neural Network efficiently with high-level TensorFlow APIs\n",
    "\n",
    "In this section we will look at 2 high-level APIs - the layer API (_ tensorflow.layers_ or _tf.layers_) and the keras API (_tensorflow.contrib.keras_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Building multilayer neural networks using TensorFlow's Layers API\n",
    "\n",
    "let's implement a multilayer percptron to classify the handwritten digits form the MNIST dataset, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!gzip ./MNIST_data/*ubyte.gz -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind)    \n",
    "    \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "#         print(magic, n)\n",
    "        labels = np.fromfile(lbpath, dtype = np.uint8)\n",
    "    \n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII', imgpath.read(16))\n",
    "#         print(magic, num, rows, cols)\n",
    "        images =np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    "        images = ((images / 255.) - .5)*2\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows 60000, Columns: 60000\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_mnist('MNIST_data/', kind='train')\n",
    "\n",
    "print('Rows %d, Columns: %d'%(X_train.shape[0], y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 10000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = load_mnist('MNIST_data/', kind='t10k')\n",
    "print('Rows: %d, columns: %d' %(X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,)\n",
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "## Mean Centering and normalization\n",
    "\n",
    "mean_vals = np.mean(X_train, axis=0)\n",
    "std_val = np.std(X_train)\n",
    "\n",
    "X_train_centered = (X_train - mean_vals)/std_val\n",
    "X_test_centered = (X_test - mean_vals)/std_val\n",
    "\n",
    "\n",
    "del X_train, X_test\n",
    "\n",
    "print(X_train_centered.shape, y_train.shape)\n",
    "print(X_test_centered.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez_compressed('mnist_centered.npz',\n",
    "                    X_train=X_train_centered,\n",
    "                   y_train=y_train,\n",
    "                   X_test=X_test_centered,\n",
    "                   y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = np.load('mnist_centered.npz')\n",
    "X_train_centered = mnist['X_train']\n",
    "y_train = mnist['y_train']\n",
    "X_test_centered = mnist['X_test']\n",
    "y_test = mnist['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can start building our mode. We will start \n",
    "# by crating two placeholder,s named _tf-x_ and _tf-y_ , but with threee fully connected layers.\n",
    "\n",
    "n_features = X_train_centered.shape[1]\n",
    "n_classes = 10\n",
    "random_seed =123\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(random_seed)\n",
    "    tf_x = tf.placeholder(dtype=tf.float32, shape=(None,n_features), \n",
    "                         name='tf_x')\n",
    "    tf_y = tf.placeholder(dtype=tf.int32, shape=None, name='tf_y')\n",
    "    \n",
    "    y_onehot = tf.one_hot(indices=tf_y, depth=n_classes)\n",
    "    \n",
    "    h1 = tf.layers.dense(inputs=tf_x, units=50, activation=tf.tanh, name='layer1')\n",
    "    h2 = tf.layers.dense(inputs=h1, units=50, activation=tf.tanh, name='layer2')\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=h2, units=10, activation=None, name='layer3')\n",
    "    \n",
    "    predictions = {\n",
    "        'classes': tf.argmax(logits, axis=1, name='predicted_classes'),\n",
    "                  'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n",
    "    }\n",
    "    \n",
    "## define cost funciton and optimizer:\n",
    "with g.as_default():\n",
    "    cost = tf.losses.softmax_cross_entropy(onehot_labels=y_onehot, logits=logits)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "    train_op = optimizer.minimize(loss=cost)\n",
    "\n",
    "    init_op  = tf.global_variables_initializer()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generator implementatio for : batch generation.\n",
    "\n",
    "def create_batch_generator(X,y, batch_size=128, shuffle=False):\n",
    "    X_copy = np.array(X)\n",
    "    y_copy = np.array(y)\n",
    "    if shuffle:\n",
    "        data = np.column_stack((X_copy, y_copy))\n",
    "        np.random.shuffle(data)\n",
    "        X_copy = data[:,:-1]\n",
    "        y_copy = data[:,-1].astype(int)\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield (X_copy[i:i+batch_size, :], y_copy[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Epoch  1 Avg. Training Loss : 0.8969\n",
      " --- Epoch  2 Avg. Training Loss : 0.4567\n",
      " --- Epoch  3 Avg. Training Loss : 0.3654\n",
      " --- Epoch  4 Avg. Training Loss : 0.3193\n",
      " --- Epoch  5 Avg. Training Loss : 0.2895\n",
      " --- Epoch  6 Avg. Training Loss : 0.2678\n",
      " --- Epoch  7 Avg. Training Loss : 0.2508\n",
      " --- Epoch  8 Avg. Training Loss : 0.2367\n",
      " --- Epoch  9 Avg. Training Loss : 0.2249\n",
      " --- Epoch 10 Avg. Training Loss : 0.2146\n",
      " --- Epoch 11 Avg. Training Loss : 0.2057\n",
      " --- Epoch 12 Avg. Training Loss : 0.1976\n",
      " --- Epoch 13 Avg. Training Loss : 0.1903\n",
      " --- Epoch 14 Avg. Training Loss : 0.1838\n",
      " --- Epoch 15 Avg. Training Loss : 0.1777\n",
      " --- Epoch 16 Avg. Training Loss : 0.1721\n",
      " --- Epoch 17 Avg. Training Loss : 0.1668\n",
      " --- Epoch 18 Avg. Training Loss : 0.1619\n",
      " --- Epoch 19 Avg. Training Loss : 0.1574\n",
      " --- Epoch 20 Avg. Training Loss : 0.1531\n",
      " --- Epoch 21 Avg. Training Loss : 0.1490\n",
      " --- Epoch 22 Avg. Training Loss : 0.1452\n",
      " --- Epoch 23 Avg. Training Loss : 0.1416\n",
      " --- Epoch 24 Avg. Training Loss : 0.1381\n",
      " --- Epoch 25 Avg. Training Loss : 0.1348\n",
      " --- Epoch 26 Avg. Training Loss : 0.1318\n",
      " --- Epoch 27 Avg. Training Loss : 0.1287\n",
      " --- Epoch 28 Avg. Training Loss : 0.1258\n",
      " --- Epoch 29 Avg. Training Loss : 0.1231\n",
      " --- Epoch 30 Avg. Training Loss : 0.1204\n",
      " --- Epoch 31 Avg. Training Loss : 0.1179\n",
      " --- Epoch 32 Avg. Training Loss : 0.1155\n",
      " --- Epoch 33 Avg. Training Loss : 0.1131\n",
      " --- Epoch 34 Avg. Training Loss : 0.1108\n",
      " --- Epoch 35 Avg. Training Loss : 0.1087\n",
      " --- Epoch 36 Avg. Training Loss : 0.1066\n",
      " --- Epoch 37 Avg. Training Loss : 0.1045\n",
      " --- Epoch 38 Avg. Training Loss : 0.1025\n",
      " --- Epoch 39 Avg. Training Loss : 0.1006\n",
      " --- Epoch 40 Avg. Training Loss : 0.0988\n",
      " --- Epoch 41 Avg. Training Loss : 0.0970\n",
      " --- Epoch 42 Avg. Training Loss : 0.0952\n",
      " --- Epoch 43 Avg. Training Loss : 0.0935\n",
      " --- Epoch 44 Avg. Training Loss : 0.0919\n",
      " --- Epoch 45 Avg. Training Loss : 0.0903\n",
      " --- Epoch 46 Avg. Training Loss : 0.0887\n",
      " --- Epoch 47 Avg. Training Loss : 0.0872\n",
      " --- Epoch 48 Avg. Training Loss : 0.0857\n",
      " --- Epoch 49 Avg. Training Loss : 0.0842\n",
      " --- Epoch 50 Avg. Training Loss : 0.0828\n"
     ]
    }
   ],
   "source": [
    "## crate a session to launc the graph\n",
    "sess = tf.Session(graph=g)\n",
    "\n",
    "## runt the varible intialization operaor\n",
    "sess.run(init_op)\n",
    "\n",
    "\n",
    "## 50 epoch of traning:\n",
    "for epoch in range(50):\n",
    "    training_costs = []\n",
    "    batch_generator = create_batch_generator(X_train_centered, y_train, shuffle=True)\n",
    "    for batch_x, batch_y in batch_generator:\n",
    "        ## prepare a dict to feed data to oru network:\n",
    "        feed = {tf_x:batch_x, tf_y:batch_y}\n",
    "        _, batch_cost = sess.run([train_op, cost], feed_dict=feed)\n",
    "        training_costs.append(batch_cost)\n",
    "    print(' --- Epoch %2d Avg. Training Loss : %.4f' %(epoch+1, np.mean(training_costs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy 95.820\n"
     ]
    }
   ],
   "source": [
    "## Do prediction on the test set:\n",
    "feed = {tf_x: X_test_centered}\n",
    "\n",
    "y_pred = sess.run(predictions['classes'], feed_dict=feed)\n",
    "\n",
    "print('Test Accuracy %.3f'  %(100* np.sum(y_pred == y_test)/y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Developing a multilayer neural network with Keras\n",
    "\n",
    "The development of kears stated in the ealru  months of 2015. Today it is one of the most popular and widely used libraties that is buildt  on top of Theano and TensorFlow.\n",
    "\n",
    "- One of the prominent featuers is that it has a very intuitive and user-friendly APT, which allows us to implement neural networks in only a few lines of code.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.keras as keras\n",
    "\n",
    "\n",
    "np.random.seed(123)\n",
    "tf.set_random_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 labels :  [5 0 4]\n",
      "\n",
      " First 3 labes (one_hot) \n",
      " [[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# To continue witht eperpatation of the training data, we need to conver the class labesl (intergers0-9) \n",
    "# into the one-hot format. \n",
    "\n",
    "y_train_onehot = keras.utils.to_categorical(y_train)\n",
    "\n",
    "print('First 3 labels : ', y_train[:3])\n",
    "\n",
    "\n",
    "print('\\n First 3 labes (one_hot) \\n' , y_train_onehot[:3,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now  we can get to tjhe interesting part and implemet a neural network, Briefly, we will have three layers, where the first two layers each have 50 hidden units with the _tanh_ activation function and the last layer has 10 units for the 10 class labesl and uses _softmax_ to give the probaility of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Dense(units=50,activation='tanh',input_dim=X_train_centered.shape[1]))\n",
    "\n",
    "model.add(keras.layers.Dense(units=50, activation='tanh', input_dim=50))\n",
    "\n",
    "model.add(keras.layers.Dense(units=y_train_onehot.shape[1], activation='softmax', input_dim=50))\n",
    "\n",
    "sgd_optimizer = keras.optimizers.SGD(lr=0.001, momentum=.9, decay=1e-7)\n",
    "\n",
    "model.compile(optimizer=sgd_optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, we initializse a new model using the _Sequential_ class to implement a feedforward neural network. Then ,we can add as may layert ot it as we like.<br>\n",
    "- However, since the first layer that we add is the input layer, we have to make sure that the _input-dim_ attribute matches the number of features(columns) in the training set(here 784)\n",
    "<br>\n",
    "- Also we have to make sure that the number of output units (_units_) and input units (_input-dim_) of two consecutive layers mathc. In the preceding example, we added two hideen layers with 50 hidden units plus one bias unit each. The number of units in the output layer should be equal to the number of unique class labesl -- the number of columns in the one-hot-encoded class label array.\n",
    "\n",
    "- Before we can compile our model, we aslo have to define an optimizer. In the preding example, we hose a stchastic gradient descent optimization, whic we are already familliar with from previous chaptes, we set the cpst (or loss) funciton to _categorical-crossentropy_.\n",
    "\n",
    "\n",
    "- The binary cross-entropy is just a techical term for the cost funciton in the logistic regression, and the categorical cross-entropy is its generalization for multiclass prediction via softmax.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We used a new intiialization algorithm for weight matrices by setting _kernel-initializer='glorot\\_uniform_'  by default.  Glorot initialization (also know as Xavier initialization) is a more robuts way ot initalization for deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "54000/54000 [==============================] - 4s - loss: 0.7247 - val_loss: 0.3616\n",
      "Epoch 2/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.3718 - val_loss: 0.2815\n",
      "Epoch 3/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.3087 - val_loss: 0.2447\n",
      "Epoch 4/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.2728 - val_loss: 0.2216\n",
      "Epoch 5/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.2475 - val_loss: 0.2042\n",
      "Epoch 6/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.2277 - val_loss: 0.1918\n",
      "Epoch 7/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.2115 - val_loss: 0.1810\n",
      "Epoch 8/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.1979 - val_loss: 0.1719\n",
      "Epoch 9/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.1860 - val_loss: 0.1646\n",
      "Epoch 10/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.1758 - val_loss: 0.1591\n",
      "Epoch 11/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.1667 - val_loss: 0.1543\n",
      "Epoch 12/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.1589 - val_loss: 0.1491\n",
      "Epoch 13/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.1516 - val_loss: 0.1451\n",
      "Epoch 14/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.1450 - val_loss: 0.1420\n",
      "Epoch 15/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.1389 - val_loss: 0.1385\n",
      "Epoch 16/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.1333 - val_loss: 0.1363\n",
      "Epoch 17/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.1283 - val_loss: 0.1331\n",
      "Epoch 18/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.1234 - val_loss: 0.1327\n",
      "Epoch 19/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.1191 - val_loss: 0.1293\n",
      "Epoch 20/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.1148 - val_loss: 0.1282\n",
      "Epoch 21/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.1109 - val_loss: 0.1270\n",
      "Epoch 22/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.1071 - val_loss: 0.1265\n",
      "Epoch 23/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.1037 - val_loss: 0.1243\n",
      "Epoch 24/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.1003 - val_loss: 0.1229\n",
      "Epoch 25/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.0971 - val_loss: 0.1216\n",
      "Epoch 26/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.0941 - val_loss: 0.1212\n",
      "Epoch 27/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.0912 - val_loss: 0.1200\n",
      "Epoch 28/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.0884 - val_loss: 0.1202\n",
      "Epoch 29/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.0858 - val_loss: 0.1189\n",
      "Epoch 30/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.0834 - val_loss: 0.1184\n",
      "Epoch 31/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.0810 - val_loss: 0.1184\n",
      "Epoch 32/50\n",
      "54000/54000 [==============================] - ETA: 0s - loss: 0.078 - 3s - loss: 0.0787 - val_loss: 0.1189\n",
      "Epoch 33/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.0765 - val_loss: 0.1183\n",
      "Epoch 34/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.0743 - val_loss: 0.1196\n",
      "Epoch 35/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.0723 - val_loss: 0.1179\n",
      "Epoch 36/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.0703 - val_loss: 0.1174\n",
      "Epoch 37/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.0684 - val_loss: 0.1184\n",
      "Epoch 38/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.0665 - val_loss: 0.1186\n",
      "Epoch 39/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.0647 - val_loss: 0.1171\n",
      "Epoch 40/50\n",
      "54000/54000 [==============================] - 4s - loss: 0.0629 - val_loss: 0.1172\n",
      "Epoch 41/50\n",
      "54000/54000 [==============================] - 4s - loss: 0.0613 - val_loss: 0.1175\n",
      "Epoch 42/50\n",
      "54000/54000 [==============================] - 4s - loss: 0.0597 - val_loss: 0.1170\n",
      "Epoch 43/50\n",
      "54000/54000 [==============================] - 4s - loss: 0.0581 - val_loss: 0.1168\n",
      "Epoch 44/50\n",
      "54000/54000 [==============================] - 4s - loss: 0.0566 - val_loss: 0.1166\n",
      "Epoch 45/50\n",
      "54000/54000 [==============================] - 4s - loss: 0.0552 - val_loss: 0.1166\n",
      "Epoch 46/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.0537 - val_loss: 0.1162\n",
      "Epoch 47/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.0523 - val_loss: 0.1170\n",
      "Epoch 48/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.0510 - val_loss: 0.1172\n",
      "Epoch 49/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.0498 - val_loss: 0.1171\n",
      "Epoch 50/50\n",
      "54000/54000 [==============================] - 3s - loss: 0.0485 - val_loss: 0.1174\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_centered, y_train_onehot,\n",
    "                    batch_size=64, epochs=50,\n",
    "                    verbose=True, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The _validation-split_ parameter is especally handy since tiwll reserve 10 % of the training data(here 6000 samples) for validation after__each epoch_ so that we can mointor whether the model is overfitting during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 predictions  [5 0 4]\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model.predict_classes(X_train_centered, verbose=0)\n",
    "print('First 3 predictions ', y_train_pred[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.71565982e-05,   5.97541337e-04,   1.47282815e-04,\n",
       "          3.38414669e-01,   5.71595571e-08,   6.59369588e-01,\n",
       "          1.61520584e-05,   6.08128263e-04,   3.87391628e-05,\n",
       "          7.60653464e-04],\n",
       "       [  9.99917269e-01,   1.00162447e-06,   1.89916664e-05,\n",
       "          3.55255088e-06,   3.59191688e-07,   2.55292616e-05,\n",
       "          2.05055630e-06,   4.14505485e-06,   2.91263314e-06,\n",
       "          2.41696744e-05],\n",
       "       [  6.18520016e-06,   1.77971931e-04,   4.03145561e-04,\n",
       "          3.20039876e-03,   9.92191255e-01,   1.04643914e-05,\n",
       "          1.01812102e-03,   2.16930569e-03,   5.41081317e-05,\n",
       "          7.69073085e-04]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting the probabilities\n",
    "y_traing_proba = model.predict_proba(X_train_centered, verbose=0)\n",
    "\n",
    "y_traing_proba[:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 predictions  [5 0 4]\n",
      "Trainig accuracy : 98.88%\n",
      "Test accuracy : 96.04%\n"
     ]
    }
   ],
   "source": [
    "# FINALLY let's print the model accuracy on traing and test sets\n",
    "\n",
    "correct_preds = np.sum(y_train == y_train_pred)\n",
    "train_acc = correct_preds/y_train.shape[0]\n",
    "\n",
    "print('First 3 predictions ', y_train_pred[:3])\n",
    "\n",
    "print('Trainig accuracy : %.2f%%' % (train_acc*100))\n",
    "\n",
    "\n",
    "y_test_pred = model.predict_classes(X_test_centered, verbose=0)\n",
    "correct_preds = np.sum(y_test_pred == y_test)\n",
    "test_acc = correct_preds/y_test.shape[0]\n",
    "\n",
    "print('Test accuracy : %.2f%%' % (test_acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Choosing activation functions for multilayer networks\n",
    "\n",
    "- Technically, we can use any function as an acitvation function in mulitlayer neural etowrks as long as it is differetiable. We can even use linear activation  functions, such as in Adaline.\n",
    "- In practice, it would not be very useful to use linear activation functions for both hidden and output layers since we want to introduce nonlinearity in a typical artificial neural network to be able ot tackle comples problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The ___logistict function__ we used probably mimics the concept of a neureon in a brain most closely -- we can think of it as the probaility of whether a neuraon fires or not_\n",
    "- However, logistic activation functions can be problematic if we have higly negative input since the output of the sigmoid function ___return output__ that are close to __zero___, the neural network would __learn very slowly and it becomes more likely that it gets trapped in the local minima during training__. This is why people often prefer a hyperbolic tangent as an activation function in hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic function recap\n",
    "\n",
    "The logistic function often just called the sigmoid function, is in fact a special case of a sigmoid function\n",
    "\n",
    "- Recall that we can se a logistic function to modelthe probability that sample _x_ belings ot the positive class (class 1 ) in a binary classification task. The net given net input _z_ is shown in the follwing equaiton:\n",
    "\n",
    "<center>$ z = w_0x_0 + w_1x_1 + w_2x_2 + .... + w_mx_m \\space = \\space \\sum_{i=0}^{m}w_ix_i \\space = \\space w^Tx $</center>\n",
    "\n",
    "The logistic function will comput the follwing:\n",
    "<center>$ \\phi_{logistic}(z) \\space = \\space \\frac {1}{1+e^{-z}} $</center>\n",
    " \n",
    "Note that $w_0$ is the bias unit (y-axis intercept, which means $x_0$ =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For MNIST data classification task, we used one-hot encoidng technique to compute the values in the output layer consisting of multiple logistic activation units.\n",
    "- However, as we will demonstrate with the following code example, an output layer cosisting of multiple logistic actvation units does not produce meaningful, interpretable porbaility values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net_input(X, W):\n",
    "    return np.dot(X,W)\n",
    "\n",
    "def logistic(z):\n",
    "    return 1.0 /(1.0 + np.exp(-z))\n",
    "\n",
    "def logistic_activation(X, W):\n",
    "    z = net_input(X,W)\n",
    "    return logistic(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net input \"\n",
      " [ 1.78  0.76  1.65]\n",
      "Output Units: \n",
      " [ 0.85569687  0.68135373  0.83889105]\n"
     ]
    }
   ],
   "source": [
    "# W : array with shape = (n_output_units  n_hidden_units +1)\n",
    "# Note that the first column are the bias units\n",
    "\n",
    "W = np.array([[1.1, 1.2, 0.8, 0.4],\n",
    "             [0.2, 0.4, 1.0, 0.2],\n",
    "             [0.6, 1.5, 1.2, 0.7]])\n",
    "\n",
    "# A : data array with shape = (n_hidden_units +1, n_samples)\n",
    "# Note that the first column of this array must be 1\n",
    "\n",
    "A = np.array([[1, 0.1, 0.4, 0.6]])\n",
    "\n",
    "Z = np.dot(W, A[0])\n",
    "y_probas = logistic(Z)\n",
    "\n",
    "\n",
    "print('Net input \"\\n', Z)\n",
    "\n",
    "print('Output Units: \\n', y_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the output, the __resulting values can not be interpeted as probabilities for a three -class problem.__ The __reason for this is that they do not sum up to 1. __\n",
    "However, this is in ffact not abig concern if we only use our model to predict the class labes, not the class membership probailities. \n",
    "- One way to preidct the class label form the output units obtianed earlier is touse the maximum value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class label : 0 \n"
     ]
    }
   ],
   "source": [
    "y_class = np.argmax(y_probas, axis=0)\n",
    "print('Predicted class label : %d ' %y_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In certain contexts, it cna be useful to ocmpute meangful class probabilities for mulitclass predicitons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating class probailiities in multiclass classification via the softmax function\n",
    "\n",
    "- The _softmax_ functio is in fact a soft form of the _argmax_ (observered above) function; instead of givieng a single class index, it provides the probaility of each class. Therefore it allows us to compute  meaningful class porbabilities in multiclass settings( multinomial logistic regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In _softmax_, the porbaility of a particular sample with net input _z_ belonging to the ith class can be computed with a normalization term in the denominator, that is the sum of all _M_ linear funcitons:\n",
    "\n",
    "<center>$ p(y=i \\space |\\space z) \\space = \\space \\phi (z) = \\frac {e^{z_i}}{\\sum_{j=1}^{M} \\space \\space e^{z_j}} $ </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities : \n",
      " [ 0.44668973  0.16107406  0.39223621]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "y_probas = softmax(Z)\n",
    "print('Probabilities : \\n', y_probas)\n",
    "\n",
    "np.sum(y_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the predicted class probabilities now sum up to 1, as we would expect. It is also ntable that the predicted class label is the same as wehn we applied the _argmax_ funciton to the logistic output. Intuitivelyu, it may help to think of the _softmax_ funciton as a _normalized_ output that is useful to obtain meaningful class-membership predicitons in multiclass settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadening the output spectrum using a hyperbolic tangent\n",
    "\n",
    "another sigmoid function that is often used in the hidden layer of aritificial nueral networks is the __hyperbolic tangent__ (commonly known as __tanh__), which can be interpreted as a rescaled versoin of the logistic functions:\n",
    "\n",
    "<center>$ \\phi_{logistic}(z) \\space = \\space \\frac {1}{1+e^{-z}} $</center>\n",
    "\n",
    "<center>$ \\phi_{tanh}(z) \\space = \\space 2 \\space x \\space \\phi_{logistic}(2z) -1 = \\frac {e^z \\space - \\space e^{-z}}{e^z \\space + \\space e^{-z}}  $</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ___advantage of the hyperbolic tangent over the logistic function is that it has broader output spectrum and ranges in the open interval (-1, 1),___ which can improve the convergence of the back progpagation alogrithm.\n",
    "<br>\n",
    "In contrast, the logistic funciton returns a n output signal that ranges in the open interval (0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0ad2de6b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tanh(z):\n",
    "    e_p = np.exp(z)\n",
    "    e_n = np.exp(-z)\n",
    "    \n",
    "    return (e_p - e_n) / (e_p + e_n)\n",
    "\n",
    "\n",
    "z = np.arange(-5, 5, 0.005)\n",
    "log_act = logistic(z)\n",
    "tanh_act = tanh(z)\n",
    "\n",
    "\n",
    "plt.ylim([-1.5,1.5])\n",
    "plt.xlabel('net input $Z$')\n",
    "plt.ylabel('activation $\\phi(z)$')\n",
    "plt.axhline(1, color='black', linestyle=':')\n",
    "plt.axhline(0.5, color='black', linestyle=':')\n",
    "plt.axhline(0, color='black', linestyle=':')\n",
    "plt.axhline(-0.5, color='black', linestyle=':')\n",
    "plt.axhline(-1, color='black', linestyle=':')\n",
    "\n",
    "plt.plot(z, tanh_act, linewidth=3, linestyle='--', label='tanh')\n",
    "plt.plot(z, log_act, linewidth=3,  label='logistic')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the shapes of the two sigmoidal curves look very similar, however the ___tanh_ function has 2x larger output space than the _logistic_ funciton__.\n",
    "<br>\n",
    "We implemented the _logistic_ and _tanh_ funcitons verbosely for the purpose of illustration. In practice we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh_act = np.tanh(Z)\n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "log_act = expit(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified linear unit activation\n",
    "\n",
    "__Rectifiled linear Units (ReLU)__ is anothe ractivation fucntion that is often used in deep neural networks. \n",
    "<br>\n",
    "\n",
    "Before we understand the ReLU, we need to understand the ___vanishing gradient problem of tanh ahd logistic activations___.<br>\n",
    "\n",
    "To understand this problem let's assume that we initally have the net input $z_1$ = 20, which changes to $z_2$ = 25. Computing the tanh acitvation, we get $\\phi (z_1)$ ~ 1.0 and $\\phi(z_2)$ ~ 1.0, which shows no change in the output.\n",
    "\n",
    "<br>\n",
    "__This means the derivative of activations with respect to net input diminshes as _z_ becomes large__. As a result, learning weights during the training phase become very slow because the gradient terms may be very close to zero. ReLU activation addresses this issue. Mathematically, ReLU is defined as follows:\n",
    "\n",
    "<center>$\\phi (z) \\space = \\space max(0, z)$</center>\n",
    "\n",
    "ReLU is stil a nonlinear function that is good for leanring comple funcitons with neural networks, Besides this , the derivative of ReLU, with respec to its input, is always 1 for positive input vlaues, Therfore, it slves the problem of vanishihg gradients, making it sutable for deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![activaiton function images](activation_fn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
